La pipeline di rendering considerata è quella basata su rasterizzazione, ma ne esistono anche altre (ad esempio, ray tracing).

La pipeline è suddivisa in 4 (macro)stadi:
1.  ***Applicazione***: vengono usate API grafiche per produrre mesh triangolari, e vengono effettuate operazioni quali animazione, accelerazione, collisioni, fisica, ecc... . Le operazioni cambiano in base al tipo di applicazione usata.
2. ***Elaborazione geometrica***: applica trasformazioni e proiezioni sui triangoli. Questo stadio calcola ciò che deve essere disegnato su schermo, ed il dove. Effettua anche calcoli per lo shading.
3. ***Rasterizzazione***: questo stadio prende le mesh triangolari e trova i pixel intersecati dai triangoli, mandandoli allo stadio successivo.
4. **Elaborazione pixels**: determina il colore dei vari pixel (shading) e determina quali oggetti sono visibili o meno in base alla loro posizione. Effettua anche operazioni per-pixel come il color blending (calcolo del colore in base al colore precedente per lo stesso pixel).

## Stadio 2: elaborazione geometrica

Lo stadio di elaborazione geometrica è a sua volta suddiviso in altri stadi:

1. **Model & View transform**: compone la scena posizionando i modelli (oggetti) nelle coordinate di riferimento (*spazio di vista*, in cui la camera è posta all'origine e diretta verso l'asse $z$ negativo) applicando trasformazioni rigide. La mappatura avviene quindi da *model space* a *world space* (spazio unico in cui sono presenti i modelli della scena).
2. **Shading vertice**: vengono usate le informazioni contenute nei vertici (normali, colore, materiali, etc...) per calcolarne il colore secondo un'equazione di shading.
3. **Proiezioni**: trasforma il volume di vista (frustum) in un cubo con estremi a $(-1,-1,-1)$ e $(1, 1, 1)$ (volume canonico di vista). Tecnicamente, è una trasformazione (mappatura 3D $\Rightarrow$ 3D).
4. **Clipping**: scarta i triangoli non appartenenti al volume di vista. I triangoli tagliati generano nuovi poligoni, spesso non triangoli: questi vengono trasformati in triangoli prima di proseguire nella pipeline. L'output di questo stadio sono i soli triangoli all'interno della vista.
5. **Screen mapping**: le coordinate delle primitive sono trasformate in *screen coordinates* (posizione degli oggetti su schermo).

### Matrice di proiezione prospettica 

L'obiettivo è proiettare lo spazio su un piano ad una distanza $d$ dal centro di proiezione (origine del punto di vista posto in $(0,0,0)$ in cui l'asse $-Z$ rappresenta la direzione della vista e l'asse $Y$ l'alto). 

In altre parole, vogliamo proiettare un punto $p=(x,y,z)$ sul piano $z=-d$, ottenendo un nuovo punto $p'=(x',y', -d)$. Questa **proiezione** è rappresentabile con la seguente matrice:

$$\begin{bmatrix} x^1 \\ y^1 \\ 1 \end{bmatrix} = 
\begin{bmatrix} d & 0 & 0 & 0 \\ 0 & d & 0 & 0 \\ 0 & 0 & -1 & 0 \end{bmatrix}
\begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix} = 
\begin{bmatrix} dx \\ dy \\ -z \end{bmatrix} =
\begin{bmatrix} -\frac{dx}{z} \\ -\frac{dy}{z} \\ 1 \end{bmatrix}$$

Dopo la moltiplicazione del vettore per la matrice si divide per la terza componente ($-z$) per ottenere 1 all'ultima posizione, in modo tale da rappresentare un punto. La profondità viene persa (e di conseguenza la matrice non è invertibile).

### Matrice di trasformazione prospettica 

$$\begin{bmatrix} x^1 \\ y^1 \\ z^1 \\ 1 \end{bmatrix} = 
\begin{bmatrix} d & 0 & 0 & 0 \\ 0 & d & 0 & 0 \\ 0 & 0 & d & 0 \\0 & 0 & -1 & 0 \end{bmatrix}
\begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix} = 
\begin{bmatrix} dx \\ dy \\ dz \\ -z \end{bmatrix} =
\begin{bmatrix} -\frac{dx}{z} \\ -\frac{dy}{z} \\ -d \\ 1 \end{bmatrix}$$

Le coordinate $x$ e $y$ sono quelle dell'immagine, mentre la coordinata $z$ diventa quella del piano immagine ($-d$).

### Proiezione vs Trasformazione
La proiezione prospettica può essere interpretata come la somma delle operazioni di trasformazione prospettica e proiezione ortogonale (che rimuove la terza componente).

Nella proiezione ortogonale i raggi di vista sono paralleli, mentre nella realtà partono da un punto unico (ad esempio la vista dell'osservatore).

Invece di avere la proiezione prospettica di un oggetto, si trasforma l'oggetto con la trasformazione prospettica (deformandolo) e poi si effettua la proiezione ortogonale, che produce produce sul piano immagine l'oggetto con la corretta prospettiva. 
In realtà, la proiezione non viene fatta, ma si mantengono tutte le coordinate fino alla fine della computazione.

> [!nota]
> Tecnicamente una matrice $4 \times 4$ effettua una trasformazione di spazio 3D in coordinate omogenee, mentre una matrice di proiezione fa calare la dimensionalità (sono quindi $3 \times 4$). 

## Pseudo-distanza

La trasformazione non preserva la $z$ (non preserva il valore della profondità, che vale sempre $-d$) e quindi non è invertibile.

$$\begin{bmatrix} x^1 \\ y^1 \\ z^1 \\ 1 \end{bmatrix} = 
\begin{bmatrix} d & 0 & 0 & 0 \\ 0 & d & 0 & 0 \\ 0 & 0 & \alpha & \beta \\0 & 0 & -1 & 0 \end{bmatrix}
\begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix} = 
\begin{bmatrix} dx \\ dy \\ \alpha z + \beta \\ -z \end{bmatrix} =
\begin{bmatrix} -\frac{dx}{z} \\ -\frac{dy}{z} \\ -(\alpha + \frac{\beta}{z}) \\ 1 \end{bmatrix}$$

I parametri $\alpha$ e $\beta$  sono scelti per normalizzare la profondità dall'intervallo [-n, -f]  (n = near, f = far, intervallo tra gli oggetti più vicini e lontani, indicati dai piani che sono le basi del frustum di vista) nell'intervallo $[-1, 1]$. 

$$\alpha = \frac {f + n}{f - n}, \beta = - \frac{2fn}{f- n}$$

La funzione è ***monotona*** (conserva ordine) e non lineare: la non linearità causa l'attribuimento da parte della funzione della classe di distanza in base a diverse granularità, in particolare oggetti vicini al near plane non sullo stesso piano potrebbero essere posti alla stessa distanza, causando compenetrazioni una volta discretizzate le distanze.

## Frustum di vista e volume di vista canonico
Solo primitive nel **frustum di vista** (piramide troncata) sono renderizzati, tutto il resto viene troncato.

Il piano rappresentante la faccia lontana (*far plane*) è posta a $z = -f$, mentre la faccia vicina (*near plane*) è posta a $z = -n$. L'angolo minimo del near plane ha coordinate $(l, b, -n)$, il massimo $(r, t, -n)$ (**r**ight, **b**ottom, **t**op, **l**eft).

Il frustum viene convertito nel **volume di vista canonico**, un cubo con angolo minimo a (-1, -1, -1) e massimo a (1, 1, 1). Gli oggetti vengono deformati, e proiettando il cubo sul piano di vista attraverso la proiezione ortogonale sull'asse $z$ ottengo l'immagine.

$z$ viene mappato in [-1, 1], ma se consideriamo il volume canonico dobbiamo mappare anche $x$ e $y$ scalandoli e traslandoli.

$$\begin{bmatrix} x^1 \\ y^1 \\ z^1 \\ 1 \end{bmatrix} = 
\begin{bmatrix} d_x & 0 & x_0 & 0 \\ 0 & d_y & y_0 & 0 \\ 0 & 0 & \alpha & \beta \\0 & 0 & -1 & 0 \end{bmatrix}
\begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix} = 
\begin{bmatrix} d_xx + x_0 z \\ d_yy + y_0 z \\ \alpha z + \beta \\ -z \end{bmatrix} =
\begin{bmatrix} -(\frac{d_xx}{z} + x_0) \\ -(\frac{d_yy}{z} + y_0) \\ -(\alpha + \frac{\beta}{z}) \\ 1 \end{bmatrix}$$

$x_0, y_0$ sono il valore della traslazione, mentre $d_x, d_y$ sono i fattori di scalatura.

Gli angoli minimo e massimo del near plane nel frustum vengono mappati negli angoli minimo e massimo del volume di vista canonico usando i parametri $x_0, y_0, d_x, d_y$ adatti.
Con un frustum simmetrico e centrato sull'asse z, i valori $x_0, y_0, d_x, d_y$ si semplificano e conseguentemente anche la matrice di trasformazione prospettica.
La definizione di questo frustum può essere fatta anche specificando $n$, $f$ e due angoli:
1. $\phi$ tra faccia dx e sx (FOV orizzontale)
2. $\theta$ tra faccia superiore e inferiore (FOV verticale)

### Z-Fighting

La non linearità della pseudo distanza aggrava il problema dello z-fighting: questo è causato dalla discretizzazione delle distanze in base alla precisione del frame buffer (16/32 bits), che a volte non permette di riconoscere distanze diverse.

## Clipping

Il clipping rimuove ogni vertice che ha coordinate fuori da [-1, 1], generando nuovi vertici per i lati dei triangoli che fuoriescono dal volume. Quest'operazione è effettuata interamente dalla GPU.
Il clipping viene fatto tra trasformazione prospettica e la divisione prospettica (trasformazione in coordinate divise normalizzate) poiché tramite dei lati non clippati potrebbero avere dei valori (zeri o negativi) che guasterebbero la divisione prospettica.