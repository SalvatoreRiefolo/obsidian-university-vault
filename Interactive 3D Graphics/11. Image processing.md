Processing sull'intera immagine può essere fatto creando una texture con l'immagine renderizzata, ed applicando questa texture ad un quad grande quanto tutto lo schermo. 
Successivamente si può usare la pipeline di rendering per applicare shaders aggiuntivi (sharpening, blurring, anti-aliasing, motion blur, depth of field...).

## Convoluzione

Una convoluzione è un'operazione tra due segnali $S$ (immagine) e $K$ (filtro).
In una convoluzione si effettua prima il **prodotto** tra un sottoinsieme di pixel di $S$ della stessa dimensione del filtro $K$, dopo di che si **sommano** i valori nella matrice risultante per ottenere il nuovo valore di un pixel nell'immagine in uscita.

Ad esempio, applicando una matrice di dimensione $N \times N$ con valori $\frac{1}{N}$ possiamo effettuare la **media** di un'area di dimensione $N \times N$ dell'immagine.

## Blur Gaussiano (filtro passa basso)

Un filtro **passa basso** attenua rapidi cambi di intensità effettuando una media pesata tra i pixel vicini. La funzione gaussiana va a zero rapidamente agli estremi e la matrice usata crea l'effetto di un filtro passa basso, che produce blurring nell'immagine.
La matrice deve essere normalizzare per non aumentare la luminosità del pixel.

### Filtro separabile
Se un filtro può essere diviso in due vettori il cui prodotto produce il filtro stesso, questo si dice **separabile**. È conveniente rappresentare un filtro tramite due vettori poiché è necessario fare solo $2N$ operazioni usando i due filtri $1 \times N$ invece che $N \times N$ se si usasse la matrice.

## Filtro affilatura (passa alto)
Riduce le componenti a bassa frequenza (con cambi di intensità lenti).

### Filtro Sobel
Consiste in due filtri, usati per identificare cambiamenti orizzontali e verticali nell'immagine (**bordi** degli oggetti). Usando i due filtri è possibile calcolare la **magnitudine** di un bordo che attraversa un certo pixel.

## Correzione del colore

Si applicano filtri per cambiare vari attributi dell'immagine, come saturazioni o shift del colore verso certe tonalità.
Ad esempio, calcolando il valore di **luminanza Y**, assegnandolo ad ogni componente RGB e moltiplicando per un certo vettore di colori, è possibile "spostare" il colore di quel pixel verso un tono seppia o un grigio.

Per modificare l'**esposizione** si moltiplica il valore dei pixel per $2^{F Stop}$, dove $F$ è una potenza di 2.

Per un filtro **colore**, si moltiplica anche per un certo colore oltre all'esposizione.

### Saturazione e contrasto

Per **saturare** un immagine si interpola linearmente tra l'immagine ed una versione in scala di grigi.
Per aumentare il **contrasto** invece si allontana il valore dei pixel dalla versione in scala di grigi

## Profondità di campo

Con la profondità di campo è possibile sfocare oggetti lontani o vicini, in base alla loro distanza dalla camera.

Un oggetto è a fuoco quando si trova vicino alla distanza focale (**focus/mid field**); quelli più vicini (nel **near field**) o lontani (nel **far field**) risultano più o meno fuori fuoco in base alla distanza dal punto focale.

Si può **campionare l'apertura** (la posizione) della camera tenendo fisso il piano focale: in questo modo dopo aver sommato le immagini rimangono a fuoco solo gli oggetti a distanza focale.
Tuttavia è necessario effettuare il rendering da tutte le posizioni campionate, ed è computazionalmente costoso.

Un altro metodo consiste nel renderizzare tre immagini spostando il near/far clipping plane: una per gli oggetti a fuoco (focus field) e due per gli oggetti oltre (far field) e prima (close field).

Si applica del blurring alle ultime due immagini e si compongono insieme (**layering**) dal retro al fronte. Il problema di questo metodo che gli oggetti che attraversano più immagini hanno un confine netto tra parte sfocata e parte a fuoco.

### Depth-driven blurring
Si fa il blurring the pixel in base al suo valore $z$: si assume che pixel vicini siano circa alla stessa profondità, quindi il campionamento per il blurring avviene nel vicinato del pixel selezionato. La profondità determina un cerchio di confusione, che indica la grandezza dell'area in cui avviene il campionamento.

## Motion blur
Rappresenta la sfocatura causata dal movimento di un oggetto nel tempo. In base al movimento della camera rispetto all'oggetto, gli elementi sfocati possono essere l'oggetto stesso (camera fissa rispetto all'oggetto) o lo sfondo (camera segue l'oggetto).

Un metodo per aggiungere il motion blur è **campionare il tempo di esposizione**: si accumulano una serie di immagini renderizzando la scena in vari momenti con camera ed oggetti riposizionati ad ogni istante, e si sovrappongono per creare il blur. Come per la profondità di campo, i render multipli sono computazionalmente costosi.

Un altro metodo consiste nell'effettuare una **media mobile** usando l'accumulation buffer per tenere in memoria una serie di frame per pixel: dopo aver riempito il buffer (ad esempio, di dimensione 8 frame) e si renderizza un'immagine per frame. Ad ogni render si scarta il frame più vecchio (devo ri-renderizzarlo e sottrarlo alla somma) e si inserisce il nuovo, e si renderizza un'altra immagine con esso: compositando le otto immagini otteniamo il blur.

Usando un **velocity buffer** e facendo gathering tra pixel vicini è possibile ottenere effetti di blur nel tempo in base alla distanza dalla camera. Il velocity buffer da informazione su quantità e direzione dello spostamento di un pixel (interpolata sui vertici), e viene usato per calcolare la quantità di blur.

Non c'è effetto parallasse: gli oggetti vicini appaiono muoversi meno degli oggetti lontani. Servirebbe un blur differenziato in base alla distanza: si può usare il depth buffer.