---
tags: 
  - complexity-information-theory
---

La complessità di Kolmogorov è definita come la lunghezza del più breve programma che produce una stringa $x$.

Formalmente, data una MdT universale $u$ la complessità di Kolmogorov di una stringa $x$ è definita come

$$K_u(x) = \min|bin(m)|$$

con $bin(m)$ la rappresentazione binaria di $m : u(m) = x$. È quindi la lunghezza minima di una macchina $m$ che simulata da $u$ produce la stringa $x$

### Proprietà

1. $K_u(x) \geq K_u(x | y)$ : intuitivamente, fornendo informazione aggiuntiva per la rappresentazione di $x$ (ad esempio la sua lunghezza) è possibile trovare una MdT meno complessa che restituisce $x$.
2. $K_u(x | y) \leq K_A(x | y) + C_A$ : si considera il caso in cui cambiamo la macchina universale $u$ con un'altra macchina universale $A$. Queste due macchine sono equivalenti a meno di una costante dipendente dalla macchina. La nozione di complessità non dipende dunque dalla scelta della macchina universale. **Dimostrazione**: esiste un programma $p_A$ per $A$ tale che $A$ su input $p_A$ e $y$ produce $x$. Si assume che $|p_A| = K_A(x | y)$, cioè che la lunghezza del programma è la complessità di Kolmogorov usando la macchina $A$. In altre parole $p_A$ è il più piccolo possibile per $A$. Dato che $u$ è universale può ricevere in input la codifica di $A, p_a, y$ e simulare la macchina $A$ su input $p_A, y$ producendo $X$. La lunghezza del programma $p_A, y$ da simulare è più grande della complessità di Kolmogorov per $u$ dato $y$, provando così la proprietà.

Un corollario che segue l'ultima proprietà definisce che la differenza tra le complessità di Kolmogorov per generare $x$ dato $y$ usando due diverse macchine universali è al più **costante**:

$$\forall x,y : |k_u(x|u) - k_A(x|y)| \leq C$$

**Lemma**: l'insieme di stringhe che hanno complessità di Kolmogorov $\lt a$ ha meno di $2^a$ elementi.

### Teoremi

1. $K(x) \leq |x| + C$. Intuitivamente posso leggere $x$ e produrlo carattere per carattere. La costante considera tutti gli altri simboli possibilmente usati dalla macchina per produrre $x$.
2. $K(x||x|) \leq |x| + C$. Come sopra, ho bisogno di sapere almeno la lunghezza per produrre $x$.
3. $K(x) \leq |x| + 2\log(x) + C$
4. $K(x) \geq |x|$

Dimostrazione dei tre teoremi da Li e Vitanyi.

## Confronto con [[Information Theory#Entropia di Shannon|l'entropia di Shannon]]

### K $\Rightarrow$ S
Si consideri il codice di Kolmogorov 

$$\varphi_K : A^*\rightarrow \{bin(m) | m = MdT\}$$

$\varphi_K(x)$ è la codifica binaria della più piccola MdT $m$ tale che $m$ produce $x$ senza input.
$\varphi_K$ è un codice UD: la macchina universale $u$ che simula $m$ è il decoder, e la sua lunghezza è $|\varphi_K(x)|= K_u(x)$.

Si prenda in considerazione l'insieme di stringhe di lunghezza $n$, $A^n$.

$$EL_n(\varphi_K) = \sum_{x \in A^n} p(x) \cdot K(x) = En(K)$$

La lunghezza media del codice è la media delle complessità di Kolmogorov sulle stringhe di lunghezza $n$, definito $En(K)$.

Sappiamo anche che $EL_n(\varphi_K) \geq H(P^n) = n \cdot H(P)$, dunque vale anche che

$$En(K) \geq n \cdot H(P)$$
$$\frac{En(K)}{n} \geq H(P)$$

cioè che la complessità di Kolmogorov media per 1 carattere è maggiore dell'entropia di Shannon.

### S $\Rightarrow$ K
Si può considerare ogni codifica come un algoritmo per produrre stringhe.
Si consideri la macchina universale che prende in input il decoder per una certa codifica $\varphi$ e la stringa $x$ codificata $\varphi(x)$. Questa macchina produce $x$ come output.
Otteniamo che $|Dec_\varphi| + |\varphi(x)| \geq K(x)$ perché questo è un programma specifico che produce $x$, non per forza il più corto.

 $$E(|Dec_\varphi| + |\varphi(x)|) \geq E_n(K(x))$$

La lunghezza media della complessità di Kolmogorov su tutte le stringhe è minore o uguale della lunghezza media dei programmi generici, come definito sopra. La lunghezza del decoder è fissa, e possiamo riscrivere la lunghezza media della codifica:

$$|Dec_\varphi| + EL_n(\varphi) \geq E_n(K(x))$$ 
$$|Dec_\varphi| + n \cdot H(P) + 1 \geq E_n(K(x))$$

Un decoder per un codice di Shannon è $K(P)$: ci basta solo sapere la distribuzione di probabilità dei caratteri.
Otteniamo quindi che 
$$K(P) + n \cdot H(P) + 1 \geq E_n(K(x)) $$

Per ottenere la complessità di Kolmogorov rispetto ad un singolo carattere si divide per $n$

$$H(P) + \frac{K(P) + 1}{n} \geq \frac{E_n(K(x))}{n} $$
