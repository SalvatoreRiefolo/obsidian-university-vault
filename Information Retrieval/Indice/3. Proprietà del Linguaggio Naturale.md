---
tags: 
  - information-retrieval
---

Per avere un indice perfetto, sarebbe necessario avere una matrice dove abbiamo un documento per riga e tutti i suoi termini nelle colonne. Tuttavia tale matrice sarebbe troppo grande e molto sparsa (tutti i documenti conterrebbero solo un piccolo sottoinsieme di tutti i termini). Infine sarebbe impossibile definire quanto un termine è importante per un documento, in quanto viene detto solo se il termine è presente o meno e non in quale quantità.

## Frequenza delle lettere 

Da del testo in linguaggio naturale è possibile estrarre alcune **proprietà statistiche** utili.

La distribuzione di frequenze delle lettere usate in un testo è simile per i testi scritti nella stessa lingua e può essere quindi utilizzato per il riconoscimento di lingua paragonando le distribuzioni con dei valori standard.

A partire da questa distribuzione è possibile anche generare del testo. Alcuni metodi sono:
- **Binomiale**: genero una lettera con probabilità pari alla sua frequenza.
- **Markoviano**: la probabilità dipende anche dalle lettere precedenti. In ogni lingua ci sono lettere che si trovano con più probabilità vicino ad altre. È possibile decidere l'**ordine** del modello, dove 0 indica il modello binomiale (casuale), 1 indica che la lettera successiva prende in considerazione quella precedente, 2 considera le due precedenti ecc... . 

## Frequenza delle parole
Usando il modello Markoviano sulle parole invece che sulle lettere, con un certo ordine si ottengono frasi quasi sintatticamente (***ma non semanticamente***) corrette.
Tuttavia è più difficile definire la frequenza delle parole in termini assoluti (la distribuzione dipende dal contesto del testo).

### Legge di Zipf
La distribuzione di frequenza delle parole segue la **legge di Zipf**, una legge di potenza.
La legge dice che poche parole appaiono spesso, e molte parole appaiono raramente. Il rank di una parola è dunque inversamente proporzionale alla sua frequenza.
Questa legge è definita da 

$$F(r) = \frac{C}{r^\alpha}$$

dove:
- $r$ è il rank, ossia la posizione della parola nell'ordinamento di frequenza.
- $\alpha \approx 1$ e $C \approx 0.1$ sono costanti che assicurano estremi non troppo lontani dalla distribuzione media.

La frequenza della r-esima parola è proporzionale a $\frac{1}{r}$.

La legge di Zipf è universale ed è mantenuta attraverso diversi testi, autori, lingue, ecc... .
Tuttavia, l'approssimazione non è perfetta e perde di precisione a frequenze alte e basse.

### Vocabolario
Un vocabolario è l'insieme dei termini unici di una collezione. Questo cresce sempre meno man mano che documenti vengono aggiunti alla collezione, in quanto molti termini sono già presenti nel vocabolario essendo stati aggiunti dai documenti precedenti.
Questo comportamento è modellato dalla **legge di Heap**.

### Legge di Heaps
La legge di Heaps è definita da

$$V = Kn^\beta$$

dove:
- $V$ è il numero di termini unici nel dizionario (la sua dimensione).
- $n$ è il numero di termini totali (dimensione del testo).
- $K \approx$ 10-100 e 0 < $\beta$ < 1 $\approx$ 0.4-0.6 sono costanti.

Il numero totale di nuovi termini in un testo di dimensione $n$ aumenta proporzionalmente a $\sqrt{n}$.

Non è stato trovato sperimentalmente un limite superiore a questa funzione $\Rightarrow$ il vocabolario per quanto poco continua ad aumentare. Questo può essere causato da neologismi, errori di battitura, multilinguismo, date, indirizzi, URLs, ecc... .
L'approssimazione data da questa legge rispetto ai dati sperimentali è molto precisa.