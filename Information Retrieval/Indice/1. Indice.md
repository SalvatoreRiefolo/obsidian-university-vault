In IR is utilizza un indice inverso per la ricerca di documenti all'interno della collezione.
Per un sistema "piccolo" basterebbe una ricerca lineare: la collezione, anche se dinamica, è caricata in memoria principale o è completamente disponibile per la ricerca. Le collezioni moderne collezioni sono molto vaste (miliardi di documenti) ed è impossibile mantenerle su memoria principale. Per questo motivo si usa l'**indicizzazione**.

In generale non è necessario che tutti i termini siano nell'indice, come quelli con meno significato (stop words). L'indice potrebbe contenere anche termini non presenti nei testi, come metadati, URLs...

È importante che i termini non solo descrivano il documento che li contiene ma aiutino anche a discriminare tra di loro i documenti: termini presenti in tutti i documenti non sono utili per discriminare, così come quelli presenti solo in pochi documenti.

A partire dalla legge di Zipf è possibile definire quale sottoinsieme della distribuzione di termini è la più utile, escludendo sia i termini più frequenti che quelli più rari.

### Osservazioni
Non è necessario che le parole siano completamente corrette perché un essere umano sia in gradi di dedurre la parola corretta ed il significato della frase. Le lettere, soprattutto iniziale e finale, sono molto più importanti della correttezza della parola stessa.

In una frase, i nomi sono la parte più importante e veicolano il senso del testo. È più facile comprendere il senso del testo avendo solo nomi e verbi rispetto ad articoli, aggettivi, avverbi, pronomi e punteggiatura.

Per questo motivo i termini da inserire nell'indice sono il risultato di una fase di [[2. Preprocessing|preprocessing]] che riduce il numero di termini e li semplifica.

### Indice come matrice
Si può immaginare un'indice come una matrice: per ogni documento (riga) sono associati dei termini (1 nella colonna di un termine se questo è presente nel documento, 0 altrimenti).
Questa matrice è molto grande e sparsa: molti dei suoi elementi sono 0. La rappresentazione tramite matrice spreca quindi molto spazio.

## Indice inverso

Indice basato sui termini, composto da due elementi:
1. Un **vocabolario**, insieme di tutte le parole che devono essere indicizzate. Contiene:
	- Il termine.
	- Il numero di documenti dove occorre (document frequency $df$).
	- Il numero di occorrenze totali.
	- Un puntatore alla lista di occorrenze.
1. Le **occorrenze**, lista di posizioni nel testo dove i termini occorrono. Contiene:
	- ID del documento.
	- Occorrenze del termine nel documento (term frequency $tf$).
	- Un puntatore ad una lista di posizioni di occorrenze. Le posizioni sono rappresentate tramite numero di caratteri/parole dall'inizio del documento.
	- Puntatore al documento successivo che contiene il termine.

Le frequenze assolute sono utili in quanto possono essere aggiornate quando la collezione cambia senza dover effettuare ricalcoli complessi.

L'indice si definisce **inverso** perché la rappresentazione passa da "**quali termini sono contenuti nel documento**" a "**quali documenti contengono il termine**". L'indicizzazione è quindi per termini e non per documenti. 

Diverse ottimizzazioni possono essere effettuate sulle occorrenze, ognuna con i suoi pro e contro. Ad esempio si possono aggiungere le occorrenze una dopo l'altra (richiede un aggiornamento dell'indice quando si aggiungono nuovi documenti) o mantenere una lista unica invece che puntare al documento successivo (richiede allungamento/accorciamento della lista).

### Requisiti di spazio
Il vocabolario è **piccolo** ([[3. Proprietà del Linguaggio Naturale#Legge di Heaps|legge di Heaps]]) e può essere contenuto in memoria principale. Può essere ridotto ulteriormente attraverso stemming o compressione.

Le occorrenze sono molte di più (lineare rispetto al numero di parole di un testo). Senza stop words, le occorrenze sono circa il 30/40% del testo.

Empiricamente l'indice usa tra il 25% ed il 50% dello spazio occupato dalla collezione.

Rispetto alla rappresentazione matriciale viene risparmiato molto spazio e vengono fornite molte più informazioni, come posizione dei termini e frequenze assolute. In più, permette la ricostruzione dei documenti.

## Compressione dell'indice

Tre componenti da comprimere:
1. La **collezione** di documenti. Questi vengono decompressi solo per essere mostrati.
2. Il **vocabolario**. Se si usa lo stesso codice di compressione dei documenti si può effettuare la ricerca usando i termini compressi. Serve però mantenere anche i termini non compresse.
3. Le **occorrenze**. Essendo le posizioni in ordine crescente si possono rappresentare attraverso differenze (**delta**), utilizzando così meno spazio per rappresentare i valori. Si può fare riferimento alle [[Posting List Compression|tecniche di compressione per liste di occorrenze]].

### Codifica di Huffman
Si assegnano parole di codice più corte simboli che occorrono più di frequente. Questo funziona molto bene poiché la distribuzione di probabilità segue la [[3. Proprietà del Linguaggio Naturale#Legge di Zipf|legge di Zipf]]. Questa compressione può essere usata sia sulla collezione che sul vocabolario, così che la ricerca possa essere effettuata su termini compressi. 
Bisogna salvare l'associazione tra termine e codice.

### Indirizzamento a blocco
Invece di memorizzare la posizione esatta del termine, si divide i documenti in blocchi e si memorizza il blocco. Così facendo i puntatori sono più piccoli e le liste sono più corte.
Tuttavia, si perdono i vantaggi delle posizioni esatte e la collezione deve essere disponibile.

I blocchi possono essere di dimensione fissa o variabile. In base al metodo usato potrebbe essere più difficile effettuare query contestuali (bisogna leggere le parti successive del documento) o recuperare l'intero documento.

## Ricerca tramite indice

La ricerca viene effettuata in 3 steps:
1. Si cerca i termini nel vocabolario. La ricerca può essere ottimizzata ordinando la lista dei termini in ordine lessicografico ed effettuando una ricerca binaria, oppure attraverso alberi o hash.
2. Si ottengono le liste delle occorrenze
3. Si processa le liste usando gli operatori necessari (booleani, contestuali, ecc.). Si possono usare varie ottimizzazione per velocizzare le operazioni, come usare la lista più corta per scremare i risultati effettuando un'intersezione ($\land$).

La **complessità temporale** della ricerca è sottolineare in $n$, anche per query complesse: $O(n^{\alpha}), \alpha \approx 0,4..0,8$.

## Costruzione dell'indice

### Trie
Struttura dati ad albero in cui la ricerca viene effettua in tempo $O(m)$, con $m$ la lunghezza della chiave da cercare. Vengono usati per individuare chiavi specifiche all'interno di un insieme. I nodi sono spesso stringhe, e gli archi sono caratteri che portano da una stringa all'altra (concatenandone i valori). Permettono la ricerca attraverso prefissi, in quanto per selezionare tutte le parole con un certo prefisso basta prendere il sotto-albero che ha per radice il nodo rappresentante il prefisso.

### Indice basato su trie

Ogni termine $k$ della collezione viene cercato nel trie, e se non viene trovato viene inserito nella struttura dati con una lista di occorrenze vuota. Dopo di che, si aggiunge la posizione dell'occorrenza alla fine della lista. 

La complessità temporale per la costruzione è $O(n)$ nel caso peggiore (il documento contiene tutti termini diversi). 
Tuttavia se la collezione è troppo grande, il trie (nello specifico le liste di occorrenze) non possono essere mantenute in memoria principale.

### Indici parziali
Si procede come per gli indici basati su trie finché la memoria principale non è piena, e si salva l'indice (parziale) $I_i$ su file. Si svuota la memoria (mantenendo il vocabolario) e si ripete fino a terminare.
Alla fine gli indici parziali sono salvati su disco e viene effettuato un **merge gerarchico**: $I_{1}\cup I_{2} = I_{1,2}, I_{3}\cup I_{4} = I_{3,4},...,I_{1,2}\cup I_{3,4} = I_{1,2,3,4},...$.
Si ottiene così un indice unico.

La complessità temporale per generare gli indici parziali è $O(n)$, ed il numero di indici parziali è $O(n/M)$, con $M$ la dimensione della memoria principale. Sono necessari $O(\log_2(n/M))$ livelli di merge. La complessità totale diventa $O(n \log_2(n/M))$

## Aggiornamento dell'indice

Attraverso indici parziali l'aggiornamento comporta il calcolo di un nuovo indice parziale ed un nuovo merge. Il costo è relativo alla dimensione dell'aggiornamento da effettuare. Lo stesso vale per la rimozione dei documenti.