
Dall'algebra lineare, la **SVD (Singular Value Decomposition)** di una matrice $M$ è:

$$M = K \cdot S \cdot D^T$$

dove:
- $K,D$ sono le matrici di autovettori di $M\cdot M^T$ e $M^{T} \cdot M$ rispettivamente.
- $D^{T}$ è la trasposta di $D$.
- $S$ è una matrice diagonale, con elementi in ordine decrescente sulla diagonale Sono gli autovalori di $M$.

Riducendo la matrice $S$, $M$ viene ridotta a sua volta.
Se $M$ è un indice, questo può essere così "compresso" rimuovendo solamente i valori più piccoli e meno importanti.
La matrice ridotta, chiamata $M_S$, contiene quindi ancora un vettore per ogni documento, ma usa meno spazio (vengono eliminate delle colonne).

Consideriamo $M$ come la rappresentazione di un indice.
Un possibile vantaggio è che lo spazio ottenuto sia più "concettuale" e meno "sintattico" (anziché $t$ termini si hanno $s$ concetti, con $s$ numero di colonne rimaste).

### Calcolo della similarità
Moltiplicando $M$ per $M^T$ otteniamo una matrice quadrata dove ogni valore rappresenta la similarità di tutte le possibili coppie di documenti. Lo stesso vale per $M_s$ ed $M_s^T$.

### Prestazioni
Non è ancora ben chiaro se LSI apporta benefici concreti.