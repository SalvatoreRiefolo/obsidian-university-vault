Si usa una funzione che assegna probabilità a sequenze di termini, il creando il **Language Model (LM)**. Il LM è unico per documento (rispetto ad una certa funzione).

Un LM è in grado di generare un insieme di termini casualmente usando le probabilità associate a questi.

Per il calcolo della similarità ci sono più possibilità:
1. Si calcola la probabilità che il LM del documento generi la query o viceversa.
2. Si calcola la distanza tra i LM di documenti e query (le loro distribuzioni di probabilità).

Per calcolare la probabilità di generazione, si può considerare la somma di tutte le probabilità dei termini della query che appaiono nel documento. 

$$sim(Q,D) \approx \sum_{q_i \in Q}\log(P(q_i | D)) $$

dove

- $Q,D$ sono query e documento.
- $q_i$ sono i termini della query.
- $P(q_i|D)$ è la probabilità che il LM di $D$ assegni il termine $q_i$

Quando un termine della query non compare nel documento (ma nella collezione) la similarità diventa **zero**: per ovviare a questo problema si considera anche la probabilità che il termine compaia nella collezione.
Questo termine ed il termine originale vengono moltiplicati per un termine di **smoothing** $\alpha$.

