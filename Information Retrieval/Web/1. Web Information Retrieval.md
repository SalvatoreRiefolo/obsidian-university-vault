L'information retrieval su web differisce dall'IR classico:
- La **dimensione della collezione** è molto maggiore.
- Il **grado di controllo** dell'utente è differente.
- La collezione **cambia frequentemente**.
- Le necessita di informazione cambiano.
- Gli utenti **non sono esperti**.
- I documenti sono collegati attraverso **ipertesto**.
- La **qualità** dei documenti varia molto.
- La collezione è **distribuita**.
- Molti altri motivi.

Alcune tecniche di IR possono essere usate sul web ma ne sono state sviluppate di nuove per affrontare meglio il dominio specifico.

Alcune delle metriche rilevanti (oltre alla rilevanza dei documenti) sono:
- Velocità di risposta
- Resilienza agli errori
- Credibilità delle fonti
- Supporto alla query

Il fine è quello di **soddisfare l'utente** sotto tutti gli aspetti.

### Tipi di necessità
Sul web le risorse che gli utenti cercano possono essere divise in:
- **Informative**, per rispondere ad una necessità di informazione.
- Per la **navigazione**, per spostarsi su una pagina specifica.
- **Transazionali**, per fare acquisti o accedere a determinati servizi.
- Altro, tutto ciò che non rientra nelle precedenti.

### Tipi di utente
In media un utente utilizza tra i 2 ed i 3 caratteri per esprimere la sua necessità: ciò mette alla prova i motori di ricerca che devono usare l'informazione limitata per ottenere i documenti rilevanti.

Inoltre gli utenti sono **eterogenei** e posseggono conoscenze e comportamenti diversi.

## Collezione

I documenti sul web sono molto **eterogenei** sotto diversi aspetti: lingua, media usato (video, testo...), formato, argomento...

La **dimensione** della collezione è molto vasta (si stimano > 50 miliardi di pagine indicizzate). Il numero di host su cui la collezione è distribuita è sullo stesso ordine di grandezza.

La collezione inoltre è **dinamica**, poiché nuovi documenti sono continuamente aggiunti, rimossi o modificati. Inoltre il contenuto dei documenti può cambiare in base a codice (php, js...) che viene eseguito quando una pagina viene visitata.

Infine molti documenti sono **duplicati** o possono essere **fuorvianti (spam)**.

### Topologia
Il web può essere rappresentato come un [[2. Grafo del web|grafo]]. Ogni pagina/documento è un nodo e da/verso di esso partono/arrivano diversi archi. La struttura è in continuo mutamento ed è difficile dedurre molte informazioni su di esso, come la presenza di componenti fortemente connesse ed il tipo di grafo.

## Architettura di un Search Engine

Un search engine ha 3 componenti principali:
1. Il **crawler**. Si occupa di estrarre dalla collezione gli oggetti da indicizzare. Per ogni URL recupera la pagina, estrae i termini e gli URL presenti ed usa questi ultimi per proseguire l'estrazione.
2. L'**indexer**. Processa i dati raccolti dal crawler e crea l'[[0. Indice#Indice inverso|indice inverso]] con le tecniche viste in precedenza.
3. il **processore di query**. Riceve le query e risponde con i documenti trovati. Fornisce anche supporto alla riformulazione ed interagisce con l'indice.

Le tre componenti devono essere particolarmente efficienti in quanto la dimensione della collezione ed il numero di utenti è molto grande.

## Ranking basato sui link

Dopo aver effettuato il ranking è difficile ordinare ulteriormente i documenti recuperati. Si utilizzano metodi legati ai **link** entranti ed uscenti dalle pagine per valutare la bontà di un certo documento. Si assume che se una pagine viene linkata da molte pagine valutate positivamente, allora anche la pagina stessa è buona. Tuttavia questo tipo di metriche possono essere sfruttate per manipolare il ranking delle pagine.