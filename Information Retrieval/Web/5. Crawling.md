Un **crawler** è un programma che recupera automaticamente pagine web. Viene usato per vari scopi all'interno di un SE, ed è fondamentale per far si che la collezione sia di qualità e aggiornata.

Qualità, coverage e "freschezza" delle pagine sono proprietà importanti da considerare quando si effettua crawling.

### Funzionamento di un crawler
Si parte con un insieme di URL di seed, si recuperano le pagine associate a quegli URL e si estraggono gli URL contenuti nelle pagine, ripetendo il processo.

I crawler possono essere divisi in:
- **Batch**: recuperano pagine fino al raggiungimento di una condizione, e ripartono da capo se la procedura viene riavviata. Utile se la collezione è statica.
- **Incrementali**: continuano a funzionare anche se la collezione è stata completamente esaminata, per poter sostituire le pagine vecchie o rimuovere quelle non importanti.

## Efficacia del crawling

Si considerano diversi **fattori** per determinare quali pagine esaminare prima ed aumentare l'efficacia del crawling.

Questi fattori dipendono da: 
1. **Pagine web**: si favoriscono le pagine che cambiano spesso, e quelle che cambiano in maggiore quantità.
2. **Carico del crawler**: si favoriscono le pagine esaminate più indietro nel tempo, le pagine più piccole e la velocità degli host dove si trovano le pagine.
3. **Search Engine**: si favoriscono le pagine più recuperate e le pagine con il PageRank più alto.
4. **Utenti**: si favoriscono le pagine più cliccate/visitate.
5. **Carico dei server**: si favoriscono pagine da server diversi rispetto all'ultimo visitato.

Alcuni di questi fattori sono indipendenti, mentre altri sono dipendenti tra loro.
In questi fattori non è considerata la **semantica**: un sito web per il meteo è molto visitato e cambia spesso, ma il contenuto che cambia non è quello maggiormente usato dagli utenti per la ricerca.