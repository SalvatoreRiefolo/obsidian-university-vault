## Indice
Per avere un indice perfetto, sarebbe necessario avere una matrice dove abbiamo un documento per riga e tutti i suoi termini nelle colonne. Tuttavia tale matrice sarebbe troppo grande e molto sparsa (tutti i documenti conterrebbero solo un piccolo sottoinsieme di tutti i termini). Infine sarebbe impossibile definire quanto un termine è importante per un documento, in quanto viene detto solo se il termine è presente o meno e non in quale quantità.

## Frequenza delle lettere 

Da del testo in linguaggio naturale è possibile estrarre alcune **proprietà statistiche** utili.

La distribuzione di frequenze delle lettere usate in un testo è simile per i testi scritti nella stessa lingua e può essere quindi utilizzato per il riconoscimento di lingua paragonando le distribuzioni con dei valori standard.

A partire da questa distribuzione è possibile anche generare del testo. Alcuni metodi sono:
- **Binomiale**: genero una lettera con probabilità pari alla sua frequenza.
- **Markoviano**: la probabilità dipende anche dalle lettere precedenti. In ogni lingua ci sono lettere che si trovano con più probabilità vicino ad altre. È possibile decidere l'**ordine** del modello, dove 0 indica il modello binomiale (casuale), 1 indica che la lettera successiva prende in considerazione quella precedente, 2 considera le due precedenti ecc... . 

## Frequenza delle parole
Usando il modello Markoviano sulle parole invece che sulle lettere, con un certo ordine si ottengono frasi quasi sintatticamente (***ma non semanticamente***) corrette.
Tuttavia è più difficile definire la frequenza delle parole in termini assoluti (la distribuzione dipende dal contesto del testo).

### Legge di Zipf
La distribuzione di frequenza delle parole segue la **legge di Zipf**, una legge di potenza.
La legge ci dice che poche parole appaiono spesso, e molte parole appaiono raramente. Il rank di una parola è dunque inversamente proporzionale alla sua frequenza.
Questa legge è definita da 

$$F(r) = \frac{C}{r^\alpha}$$

dove:
- $r$ è il rank, ossia la posizione della parola nell'ordinamento di frequenza.
- $\alpha \approx 1$ e $C \approx 0.1$ sono costanti che assicurano estremi non troppo lontani dalla distribuzione media.

La frequenza della r-esima parola è proporzionale a $\frac{1}{r}$.

La legge di Zipf è universale ed è mantenuta attraverso diversi testi, autori, lingue, ecc... .
Tuttavia, l'approssimazione non è perfetta e perde di precisione a frequenze alte e basse.

### Vocabolario
Un vocabolario è l'insieme dei termini unici di una collezione. Questo cresce sempre meno man mano che documenti vengono aggiunti alla collezione, in quanto molti termini sono già presenti nel vocabolario essendo stati aggiunti dai documenti precedenti.
Questo comportamento è modellato dalla **legge di Heap**.

### Legge di Heap
La legge di Heap è definita da

$$V = Kn^\beta$$

dove:
- $V$ è il numero di termini unici nel dizionario (la sua dimensione).
- $n$ è il numero di termini totali (dimensione del testo).
- $K \approx$ 10-100 e 0 < $\beta$ < 1 $\approx$ 0.4-0.6 sono costanti.

Il numero totale di nuovi termini in un testo di dimensione $n$ aumenta proporzionalmente a $\sqrt{n}$.

Non è stato trovato sperimentalmente un limite superiore a questa funzione $\Rightarrow$ il vocabolario per quanto poco continua ad aumentare. Questo può essere causato da neologismi, errori di battitura, multilinguismo, date, indirizzi, URLs, ecc... .
L'approssimazione data da questa legge rispetto ai dati sperimentali è molto precisa

## Costruzione di un indice

In generale non è necessario che tutti i termini siano nell'indice, come quelli con meno significato (stopwords). L'indice potrebbe contenere anche termini non presenti nei testi, come metadati, URLs...

È importante che i termini non solo descrivano il documento che li contiene ma aiutino anche a discriminare tra di loro i documenti: termini presenti in tutti i documenti non sono utili per discriminare, così come quelli presenti solo in pochi documenti.

A partire dalla legge di Zipf è possibile definire quale sottoinsieme della distribuzione di termini è la più utile, escludendo sia i termini più frequenti che quelli più rari.

### Osservazioni
Non è necessario che le parole siano completamente corrette perché un essere umano sia in gradi di dedurre la parola corretta ed il significato della frase. Le lettere, soprattutto iniziale e finale, sono molto più importanti della correttezza della parola stessa.

In una frase, i nomi sono la parte più importante e veicolano il senso del testo. È più facile comprendere il senso del testo avendo solo nomi e verbi rispetto ad articoli, aggettivi, avverbi, pronomi e punteggiatura.